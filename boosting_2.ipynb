{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54eed811-54ff-4aac-85a0-91f4dbd40a3e",
   "metadata": {},
   "source": [
    "## Q1.  Gradient boosting is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c20bc3-96ab-4fe2-967b-b06c1faf9402",
   "metadata": {},
   "source": [
    "## Q2. code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06542bce-b384-4b1d-8c05-be2d5c2f1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f26e06-8aed-4d11-b241-e823d1a41d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=1, noise=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c0527d-f6ff-480b-8844-7307be627468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions with the mean of y\n",
    "        self.mean = np.mean(y)\n",
    "        predictions = np.full(len(y), self.mean)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals)\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Fit a weak learner (decision tree) to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=1)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the predictions by adding the weak learner's predictions\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Save the weak learner\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing the weak learners' predictions\n",
    "        predictions = np.full(len(X), self.mean)\n",
    "        for tree in self.estimators:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96eee43b-54d9-4767-91c1-92b6d99507c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4547347-3620-4709-b53c-122413aa40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414431e1-0fb0-423c-b13d-586bfb94ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1677.0785521383245\n",
      "R-squared: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = gb.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0aaaf1-2616-4359-94ce-a99ed300409d",
   "metadata": {},
   "source": [
    "## Q3. code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb225b9-0c93-4ba8-a123-e4bc01d4ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a50b52a-9167-471b-9dc6-8513a6d9431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gradient boosting regressor\n",
    "gb = GradientBoostingRegressor('n_estimators','learning_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cffccc24-7396-4876-8c81-bb0ed4ae278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "#grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdaa96bd-9faf-4e3d-ba8b-4c8ba0ff19e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': [50, 100, 150], 'learning_rate': [0.1, 0.05, 0.01], 'max_depth': [1, 2, 3]}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mparam_grid)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the best model's performance\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m      7\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y, y_pred)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.param_grid)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Best Mean Squared Error:\", mse)\n",
    "print(\"Best R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba9feb2-9f2a-4802-aa8a-d08e35750085",
   "metadata": {},
   "source": [
    "## Q4.  Decision trees are used as the weak learner in gradient boosting. Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558cc97-efca-4e23-954b-56a2ef2b6a26",
   "metadata": {},
   "source": [
    "## Q5.  In gradient boosting, we predict and adjust our predictions in the opposite (negative gradient) direction. This achieves the opposite (minimize the loss). Since, the loss of a model inversely relates to its performance and accuracy, doing so improves its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c5542-464c-407a-a563-94353aadd3e5",
   "metadata": {},
   "source": [
    "## Q6.  Sequential Ensemble Learning\n",
    "It is a boosting technique where the outputs from individual weak learners associate sequentially during the training phase. The performance of the model is boosted by assigning higher weights to the samples that are incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac03b0cd-4dd5-4126-a561-737c5f3d2eef",
   "metadata": {},
   "source": [
    "## Q7.  Steps in Gradient Boosting\n",
    "Step1: Taking the average values of Output Feature. ...\n",
    "Step 2:Calculate the residual value. ...\n",
    "Step 3: Generating the Base Learner decision Tree. ...\n",
    "Step 4: Generating Predicted output for Decision Tree1. ...\n",
    "Step 5: Predicting new o/p value for each sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
