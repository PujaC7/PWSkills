{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9515230c-6e23-4115-9b0f-2fe1698a586f",
   "metadata": {},
   "source": [
    "## Q1.  K-Nearest Neighbors Algorithm. The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e9075-2354-4826-90aa-850c97b0e106",
   "metadata": {},
   "source": [
    "## Q2.   The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa4828-895c-4590-aa45-50b7e331c9db",
   "metadata": {},
   "source": [
    "## Q3.  The key differences are: KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098921f-1f5c-40fc-a8af-47faa2d3ffd7",
   "metadata": {},
   "source": [
    "## Q4.  1. Evaluation procedure 1 - Train and test on the entire dataset\n",
    "- Train the model on the entire dataset.\n",
    "- Test the model on the same dataset, and evaluate how well we did by comparing the predicted response values with the true response values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d157788-470a-4fe9-a7dc-fa8b517366a0",
   "metadata": {},
   "source": [
    "## Q5.  The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddb4fc-778c-4d54-8550-bad0b6272466",
   "metadata": {},
   "source": [
    "## Q6.  Python | Imputation using the KNNimputer()\n",
    "KNNimputer is a scikit-learn class used to fill out or predict the missing values in a dataset. It is a more useful method which works on the basic approach of the KNN algorithm rather than the naive approach of filling all the values with mean or the median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c9b65-9125-46b3-b88d-eb4b3826edaf",
   "metadata": {},
   "source": [
    "## Q7.  KNN Classifier is better suited for classification problems where the decision boundary is non-linear or complex. It can be effective when the classes have overlapping regions. KNN classifier can be used when the data is not highly dimensional and when there is no clear separation between classes.\n",
    "\n",
    "## KNN Regressor is better suited for regression problems where the relationship between the features and the target variable is non-linear and complex. It can handle situations where the data points exhibit local patterns and can be used when there is no prior assumption about the underlying data distribution.\n",
    "\n",
    "It's important to note that the performance of KNN classifier and regressor can be influenced by the choice of the number of neighbors (k) and the distance metric used. Therefore, it's advisable to perform parameter tuning and feature scaling to optimize the performance of the algorithm for a specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c15fe-d801-4a85-b51c-d1d173336594",
   "metadata": {},
   "source": [
    "## Q8.  Advantages and disadvantages of KNN\n",
    "- It's easy to understand and simple to implement.\n",
    "- It can be used for both classification and regression problems.\n",
    "- It's ideal for non-linear data since there's no assumption about underlying data.\n",
    "- It can naturally handle multi-class cases.\n",
    "- It can perform well with enough representative data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661243c-eba1-4781-bb52-129ae7136125",
   "metadata": {},
   "source": [
    "## Q9.  Euclidean distance is the shortest path between source and destination which is a straight line. but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26378d-5fc8-4d28-ab4a-105a4e65eb92",
   "metadata": {},
   "source": [
    "## Q10.  Feature scaling is essential for machine learning algorithms that calculate distances between data. If not scaled, the feature with a higher value range starts dominating when calculating distances. KNN which uses Euclidean distance is one such algorithm which essentially require scaling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
