{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2341d31d-b1a0-4025-8121-4a1de77bba39",
   "metadata": {},
   "source": [
    "## Q1.  Web scraping is the process of using bots to extract content and data from a website. Unlike screen scraping, which only copies pixels displayed onscreen, web scraping extracts underlying HTML code and, with it, data stored in a database. The scraper can then replicate entire website content elsewhere.\n",
    "### Some of the main use cases of web scraping include price monitoring, price intelligence, news monitoring, lead generation, and market research among many others.\n",
    "### Web scraping typically extracts large amounts of data from websites for a variety of uses such as price monitoring, enriching machine learning models, financial data aggregation, monitoring consumer sentiment, news tracking, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45fdb8-2255-4742-8968-be5841b1f5eb",
   "metadata": {},
   "source": [
    "## Q2.  Web Scraping Techniques\n",
    "1. Human copy-and-paste.\n",
    "2. Text pattern matching.\n",
    "3. HTTP programming.\n",
    "4. HTML parsing.\n",
    "5. DOM parsing.\n",
    "6. Vertical aggregation.\n",
    "7. Semantic annotation recognizing.\n",
    "8. Computer vision web-page analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2d15b-958c-4aa8-99e4-2b9831af3938",
   "metadata": {},
   "source": [
    "## Q3. Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0aee2-f233-4eb0-88c2-82ca1df1d393",
   "metadata": {},
   "source": [
    "## Q4.  Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00063cd9-aab5-4408-8503-ac129e981647",
   "metadata": {},
   "source": [
    "## Q5.  the names of AWS services used in web scrapping\n",
    "### Amazon IAM (Identity and Access Management)\n",
    "#### Amazon IAM (Identity and Access Management) allows users to securely access and manage resources. To achieve complete access to the tools and resources provided by AWS, AWS IAM is the best AWS service. It gives you the right to have control over who has authorization (signed in) and authentication (has permissions) access to the resources. It comes with attribute-based access control which helps you to create separate permissions on the basis of the userâ€™s attributes such as job role, department, etc. Through this, you can allow or deny access given to users. AWS IAM has complete access or is a central manager for refining permissions across AWS. He/She handles who can access what.\n",
    "\n",
    "### Amazon Lambda\n",
    "#### Another promising service by AWS is Amazon Lambda which is a serverless and event-driven computing service that lets you run code for virtual applications or backend services automatically. You need to worry about servers and clusters when working with solutions using Amazon Lambda. It is also cost-effective where you have to only pay for the services you use. As a user, your responsibility is to just upload the code and Lambda handles the rest. Using Lambda, you get precise software scaling and extensive availability. With hundreds to thousands of workloads per second, AWS Lambda responsibly handles code execution requests. It is one of the best services provided by AWS for developers.\n",
    "\n",
    "### Dynamo DB\n",
    "#### DynamoDB is a serverless, document database key-value NoSQL database that is designed to run high-performance applications. It can manage up to 10 trillion requests on a daily basis and support thresholds of more than 20 million requests per second. DynamoDB has built-in security with a fully-managed multi-master, multi-region, durable database, and in-memory archiving for web-scale applications. It has in-built tools which is used to generate actionable insights, useful analytics, and monitor traffic trends. It offers built-in security, continuous backups, automated multi-region replication, data import and export, and in-memory caching. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
