{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141a6d72-517a-4673-8fad-3e318b422b29",
   "metadata": {},
   "source": [
    "## Q1.  In the context of a neural network, a neuron is the most fundamental unit of processing. It's also called a perceptron. A neural network is based on the way a human brain works. So, we can say that it simulates the way the biological neurons signal to one another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a0f6d-76ca-4f22-bf02-c7c7ecef30c1",
   "metadata": {},
   "source": [
    "## Q2. A neuron has three main parts: dendrites, an axon, and a cell body or soma (see image below), which can be represented as the branches, roots and trunk of a tree, respectively. A dendrite (tree branch) is where a neuron receives input from other cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e5b64-7312-461e-bad9-9b6656ec6c10",
   "metadata": {},
   "source": [
    "## Q3.  The perceptron model begins with multiplying all input values and their weights, then adds these values to create the weighted sum. Further, this weighted sum is applied to the activation function 'f' to obtain the desired output. This activation function is also known as the step function and is represented by 'f. '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e510229-517b-4b22-a27b-da0a020aaf7c",
   "metadata": {},
   "source": [
    "## Q4.  A perceptron is a simple type of neural network that can learn to classify linearly separable patterns. It consists of a single layer of weighted inputs and a binary output. A multi-layer perceptron (MLP) is a more complex type of neural network that can learn to classify non-linearly separable patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5337b-94be-4a2e-9623-92e052b55528",
   "metadata": {},
   "source": [
    "## Q5.  Forward propagation is where input data is fed through a network, in a forward direction, to generate an output. The data is accepted by hidden layers and processed, as per the activation function, and moves to the successive layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d222a-c6db-48a8-afe9-edad632b5ec8",
   "metadata": {},
   "source": [
    "## Q6.  Backpropagation is a process involved in training a neural network. It involves taking the error rate of a forward propagation and feeding this loss backward through the neural network layers to fine-tune the weights. Backpropagation is the essence of neural net training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8128f0c-dfba-480d-a341-d97adb0f25fd",
   "metadata": {},
   "source": [
    "## Q7.  The chain rule can be generalised to multivariate functions, and represented by a tree diagram. The chain rule is applied extensively by the backpropagation algorithm in order to calculate the error gradient of the loss function with respect to each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d9e9b-62ad-43db-8aa5-c83b0ba1f1dc",
   "metadata": {},
   "source": [
    "## Q8.  A loss function measures how good a neural network model is in performing a certain task, which in most cases is regression or classification. We must minimize the value of the loss function during the backpropagation step in order to make the neural network better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3b9d9-7093-4387-99e5-c31ae8432c1e",
   "metadata": {},
   "source": [
    "## Q9.  Examples include mean squared error (MSE), mean absolute error (MAE), and binary cross-entropy. Often the average or sum of individual loss values in the training set. Used to evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3adf88-bb55-4068-9dc0-8be4533ab833",
   "metadata": {},
   "source": [
    "## Q10.  An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss. But it is a daunting task to choose the appropriate weights for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba14df1d-6855-42f0-997f-b5e8585d52d8",
   "metadata": {},
   "source": [
    "## Q11.  These gradients are used to update the weights. If the gradients are large, the multiplication of these gradients will become huge over time. This results in the model being unable to learn and its behavior becomes unstable. This problem is called the exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699f1eb-84de-47d4-8512-bbb6680958f2",
   "metadata": {},
   "source": [
    "## Q12.  Vanishing gradient problem is a phenomenon that occurs during the training of deep neural networks, where the gradients that are used to update the network become extremely small or \"vanish\" as they are backpropogated from the output layers to the earlier layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229528e-b40d-434e-b283-e55519fd4fa4",
   "metadata": {},
   "source": [
    "## Q13.  Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9b10d-acc0-451a-af2d-048cb71be922",
   "metadata": {},
   "source": [
    "## Q14.  Normalization can help training of our neural networks as the different features are on a similar scale, which helps to stabilize the gradient descent step, allowing us to use larger learning rates or help models converge faster for a given learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadd690-a7bd-4398-b51d-e9bbe6b3e76c",
   "metadata": {},
   "source": [
    "## Q15.  Activation Functions in Neural Networks\n",
    "- Linear or Identity Activation Function.\n",
    "- Non-linear Activation Function.\n",
    "- Sigmoid or Logistic Activation Function.\n",
    "- Tanh or hyperbolic tangent Activation Function.\n",
    "- ReLU (Rectified Linear Unit) Activation Function.\n",
    "- Leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e551b-7c95-4f9c-8c32-e65bb703b563",
   "metadata": {},
   "source": [
    "## Q16.  Batch normalization is a technique to standardize the inputs to a network, applied to ether the activations of a prior layer or inputs directly. Batch normalization accelerates training, in some cases by halving the epochs or better, and provides some regularization, reducing generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f903a36-1b61-4090-9c17-c18fa727b098",
   "metadata": {},
   "source": [
    "## Q17.  While building and training neural networks, it is crucial to initialize the weights appropriately to ensure a model with high accuracy. If the weights are not correctly initialized, it may give rise to the Vanishing Gradient problem or the Exploding Gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb417d-2518-482c-98dc-e133a54d74df",
   "metadata": {},
   "source": [
    "## Q18.  Momentum aids in the optimization process's convergence by keeping the optimizer going in the same direction as previously, even if the gradient changes direction or becomes zero. This means that the optimizer can take greater steps toward the cost function's minimum, which can help it get there faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490eb51-9882-46bf-8876-1cf6f972518b",
   "metadata": {},
   "source": [
    "## Q19.  L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights. The L1 regularization solution is sparse. The L2 regularization solution is non-sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae09f0b-e105-4dd6-954a-975c21680502",
   "metadata": {},
   "source": [
    "## Q20.  Regularization by early stopping can be done either by dividing the dataset into training and test sets and then using cross-validation on the training set or by dividing the dataset into training, validation and test sets, in which case cross-validation, is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3239dec-5144-4bdf-8d59-adb69d19e35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
