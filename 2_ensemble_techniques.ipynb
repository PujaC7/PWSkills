{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cd23c4-18e5-4d54-8f8a-24245d82c993",
   "metadata": {},
   "source": [
    "## Q1. Bagging attempts to reduce the chance of overfitting complex models. It trains a large number of “strong” learners in parallel. A strong learner is a model that's relatively unconstrained. Bagging then combines all the strong learners together in order to “smooth out” their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a65c2-628f-4bab-8df1-4effaad13c1b",
   "metadata": {},
   "source": [
    "## Q2.  Bagging offers the advantage of allowing many weak learners to combine efforts to outdo a single strong learner. It also helps in the reduction of variance, hence eliminating the overfitting of models in the procedure. One disadvantage of bagging is that it introduces a loss of interpretability of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d0452-67a1-4115-ab70-f0d6a904b339",
   "metadata": {},
   "source": [
    "## Q3.  Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance. In Bagging, each model receives an equal weight. In Boosting, models are weighed based on their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ee51e-adef-4e3c-953e-59bf1b6fdf39",
   "metadata": {},
   "source": [
    "## Q4. K-nearest neighbors (kNN) is a supervised learning algorithm that can be used to solve both classification and regression tasks. The main idea behind kNN is that the value or class of a data point is determined by the data points around it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063cb55-546d-4da6-8fc6-cb11550f5d99",
   "metadata": {},
   "source": [
    "## Q5. Bagging is a powerful ensemble method which helps to reduce variance, and by extension, prevent overfitting. Ensemble methods improve model precision by using a group (or \"ensemble\") of models which, when combined, outperform individual models when used separately.\n",
    "### There are no restrictions/guidelines on the number of models. You can start even from 3 models. You can keep the number of models as a hyperparameter if the training cost is less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81cf4c-8725-4967-8aad-a810b98f5460",
   "metadata": {},
   "source": [
    "## Q6.  Random Forest is an example of bagging ensemble learning. In the Random Forest algorithm, the base learners are only Decision Trees. Random Forest uses bagging along with column sampling to form a robust model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621daf32-ccfc-450e-94c9-0974a61bb756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
