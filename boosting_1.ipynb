{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816a040c-e6ea-43bd-9ab3-92d87ebafe69",
   "metadata": {},
   "source": [
    "## Q1.   Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba9687-c3ac-4785-9e0e-1e357b630491",
   "metadata": {},
   "source": [
    "## Q2.  Pros and Cons of Boosting\n",
    "### Boosting is a resilient method that curbs over-fitting easily. One disadvantage of boosting is that it is sensitive to outliers since every classifier is obliged to fix the errors in the predecessors. Thus, the method is too dependent on outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013628d8-7693-4d22-85c8-a183c2b2db63",
   "metadata": {},
   "source": [
    "## Q3.  Boosting creates an ensemble model by combining several weak decision trees sequentially. It assigns weights to the output of individual trees. Then it gives incorrect classifications from the first decision tree a higher weight and input to the next tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6a1a8-e654-410b-ae9c-9099c15706dd",
   "metadata": {},
   "source": [
    "## Q4.  4 Boosting Algorithms in Machine Learning\n",
    "- Gradient Boosting Machine (GBM)\n",
    "- Extreme Gradient Boosting Machine (XGBM)\n",
    "- LightGBM.\n",
    "- CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071775f-7549-4cec-9af7-b1caf0c39397",
   "metadata": {},
   "source": [
    "## Q5.  GBM (Boosted Models) Tuning Parameters\n",
    "- n. trees – Number of trees (the number of gradient boosting iteration) i.e. N. ...\n",
    "- interaction.depth (Maximum nodes per tree) - number of splits it has to perform on a tree (starting from a single node). ...\n",
    "- Shrinkage (Learning Rate) – It is considered as a learning rate. ...\n",
    "- n. ...\n",
    "- bag. ...\n",
    "- train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da7563-9bec-432f-ad1c-5d51c27d941d",
   "metadata": {},
   "source": [
    "## Q6.  Boosting algorithms combine multiple weak learners in a sequential method, which iteratively improves observations. This approach helps to reduce high bias that is common in machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bea598-3b51-40ca-85ca-9beece8135aa",
   "metadata": {},
   "source": [
    "## Q7. AdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique used as an Ensemble Method in Machine Learning. It is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5021e-4132-41e6-a74c-cb7ca581817a",
   "metadata": {},
   "source": [
    "## Q8.  The error function that AdaBoost uses is an exponential loss function. First we find the products between the true values of training samples and the overall prediction for each sample. Then we take the sum of all the exponentials of these products in order to compute the error at iteration m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf03d674-425f-4aa6-8e37-3fe45d902363",
   "metadata": {},
   "source": [
    "## Q9.  This is done by making misclassified cases to be updated with increased weights after an iteration. Increased weights would make our learning algorithm pay higher attention to these observations in the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc73b9-3207-4d36-9926-3458ed7bf9af",
   "metadata": {},
   "source": [
    "## Q10.  An important hyperparameter for Adaboost is n_estimator. Often by changing the number of base models or weak learners we can adjust the accuracy of the model. The number of trees added to the model must be high for the model to work well, often hundreds, if not thousands"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
