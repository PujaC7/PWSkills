{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f79bc9-004c-4aff-ba9c-0ea619bb40c7",
   "metadata": {},
   "source": [
    "## general linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bd135-b71b-4d23-a236-0599075eb2c1",
   "metadata": {},
   "source": [
    "## Q1.  The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b9b28-a010-49f8-bd9a-e30d298ecf9c",
   "metadata": {},
   "source": [
    "## Q2.  The general linear model fitted using ordinary least squares (which includes Student's t test, ANOVA, and linear regression) makes four assumptions: linearity, homoskedasticity (constant variance), normality, and independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0404c7-8880-450b-93df-d6b694d82c4c",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d268c-b1d1-4b18-bfe5-c0c02f5ff29c",
   "metadata": {},
   "source": [
    "## Q11.  Regression analysis is a statistical method that shows the relationship between two or more variables. Usually expressed in a graph, the method tests the relationship between a dependent variable against independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056cafb-5069-43a7-bb4c-b3e7f5e6c226",
   "metadata": {},
   "source": [
    "## Q12.  Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. For instance, when we predict rent based on square feet alone that is simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ba398-7bad-43e0-a205-7c28523e5cab",
   "metadata": {},
   "source": [
    "##  Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba1ba4-ad4a-4ade-8ab8-c4f1f0d044c9",
   "metadata": {},
   "source": [
    "## Q21.  A loss function is a mathematical function that quantifies the difference between predicted and actual values in a machine learning model. It measures the model's performance and guides the optimization process by providing feedback on how well it fits the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f095a-44d4-4011-9864-d85dadd4f351",
   "metadata": {},
   "source": [
    "## Q22.  The difference between a convex and non-convex loss function lies in their shape and properties.\n",
    "\n",
    "A convex loss function has a specific characteristic where any two points on the loss function curve lie above the line segment connecting those two points. Mathematically, a function f(x) is considered convex if, for any two points x1 and x2 in its domain, and for any value t between 0 and 1, the following condition holds:\n",
    "\n",
    "f(tx1 + (1 - t)x2) ≤ tf(x1) + (1 - t)f(x2)\n",
    "\n",
    "In simpler terms, this means that a straight line connecting any two points on the curve will lie above the curve itself. Convex loss functions have a single global minimum, which makes optimization relatively straightforward since gradient descent algorithms can converge to the global minimum efficiently.\n",
    "\n",
    "On the other hand, a non-convex loss function does not satisfy this property. It means that there exist points on the function curve where a straight line connecting two points may lie below the curve. Non-convex loss functions can have multiple local minima, making optimization more challenging. The presence of multiple local minima means that gradient-based optimization methods can get stuck in suboptimal solutions instead of reaching the global minimum.\n",
    "\n",
    "The choice of a convex or non-convex loss function depends on the problem at hand. Convex loss functions are often preferred in optimization tasks because they guarantee convergence to the global minimum and provide a unique solution. Non-convex loss functions, although more complex, can capture more intricate relationships in the data and may be suitable for certain problem domains. However, finding the global minimum for non-convex functions generally requires more sophisticated optimization algorithms and may involve trade-offs between computational complexity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db020f3f-750e-41fa-8c46-28417d92cb99",
   "metadata": {},
   "source": [
    "## Optimizer (GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca5c52-bdd1-4d18-8c76-5797b0244710",
   "metadata": {},
   "source": [
    "## Q31.  An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss. But it is a daunting task to choose the appropriate weights for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a7cf4-526c-4ca3-b499-5fc03bc65ba5",
   "metadata": {},
   "source": [
    "## Q32.  Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dba236-62bc-4e4b-beb5-10b54c559d5f",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84924867-e3c1-4078-8c5e-efdb12145342",
   "metadata": {},
   "source": [
    "## Q41.  Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976b5fc-3218-4e93-abca-1ab67c34fd6e",
   "metadata": {},
   "source": [
    "## Q42.  The differences between L1 and L2 regularization:\n",
    "### L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59505ca5-8a4f-4ee1-8a1d-29045246f24c",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281a617-3e2b-445c-a33e-a1ee51b96c03",
   "metadata": {},
   "source": [
    "## Q51.  SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. A separator between the categories is found, then the data are transformed in such a way that the separator could be drawn as a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326ec80-a681-44c6-833e-28fa8875a39e",
   "metadata": {},
   "source": [
    "## Q52.  The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations x (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations ϕ(x) and representing the data by these transformed ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ce24f-11e6-47af-a102-736f5218d0db",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf735fd-47d9-448f-adb6-51cb9fd2699e",
   "metadata": {},
   "source": [
    "## Q61.  A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49c52a-0bc5-4537-8de6-4a3d80b3218c",
   "metadata": {},
   "source": [
    "## Q62.  Information Gain = 1-Entropy\n",
    "- For each split, calculate the entropy of each child node independently.\n",
    "- Calculate the entropy of each split using the weighted average entropy of child nodes.\n",
    "- Choose the split with the lowest entropy or the greatest gain in information.\n",
    "- Repeat these steps to obtain homogeneous split nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed1a3f-6cf0-4ac3-9500-2d848603613f",
   "metadata": {},
   "source": [
    "## Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1b108-1d95-4c22-a30c-1c4c408d25eb",
   "metadata": {},
   "source": [
    "## Q71.  Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model . To better understand this definition lets take a step back into ultimate goal of machine learning and model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e3daf-1f88-4a78-8a14-2582d95d38cb",
   "metadata": {},
   "source": [
    "## Q72.  Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. In bagging, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214572c3-1825-453a-9399-4fa2745b388f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
