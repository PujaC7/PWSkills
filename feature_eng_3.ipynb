{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71351d8d-7040-4be2-a4d8-7717251b2391",
   "metadata": {},
   "source": [
    "## Q1.  Rescaling (min-max normalization) Also known as min-max scaling or min-max normalization, rescaling is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data.\n",
    "### MinMaxScaler. Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "### example: if we have 99 values between 0 and 40, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.4. That data is just as squished as before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0f6d9-9d2b-4416-a556-88340800d091",
   "metadata": {},
   "source": [
    "## Q2.  Scaling is done considering the whole feature vector to be of unit length. This usually means dividing each component by the Euclidean length of the vector (L2 Norm). In some applications (e.g., histogram features), it can be more practical to use the L1 norm of the feature vector.\n",
    "### MIN MAX scaling vs normalization: Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling. Here, Xmax and Xmin are the maximum and the minimum values of the feature, respectively\n",
    "### Example, if we have weight of a person in a dataset with values in the range 15kg to 100kg, then feature scaling transforms all the values to the range 0 to 1 where 0 represents lowest weight and 1 represents highest weight instead of representing the weights in kgs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4655e3d-ac20-4560-a67f-73f8e0d2847c",
   "metadata": {},
   "source": [
    "## Q3.  Principal Component Analysis is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation.\n",
    "### PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one\n",
    "### PCA is mainly used as the dimensionality reduction technique in various AI applications such as computer vision, image compression, etc. It can also be used for finding hidden patterns if data has high dimensions. Some fields where PCA is used are Finance, data mining, Psychology, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642755b9-8124-46db-bb11-673a40329f7b",
   "metadata": {},
   "source": [
    "## Q4.  PCA is an important method for feature extraction and image representation. In PCA, matrix transformation of the image takes place into high dimension vectors and its covariance matrix is obtained consuming high-dimension vector space.\n",
    "### PCA Algorithm for Feature Extraction: The PCA calculates a new projection of the given data set representing one or more features. The new axes are based on the standard deviation of the value of these features.\n",
    "### PCA in machine learning is used to visualize multidimensional data. In healthcare data to explore the factors that are assumed to be very important in increasing the risk of any chronic disease. PCA helps to resize an image. PCA is used to analyze stock data and forecasting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401f344-eb9e-49ca-87e1-d4410af29cef",
   "metadata": {},
   "source": [
    "## Q5. steps to use the min max scaling on the dataset\n",
    "1. Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. ...\n",
    "2. Apply the scale to training data. ...\n",
    "3. Apply the scale to data going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923baa05-779b-43f4-9ab6-6426ab3a3896",
   "metadata": {},
   "source": [
    "## Q6.  steps to use PCA to reduce the dimensionality of the dataset\n",
    "1. Standardize the d-dimensional dataset.\n",
    "2. Construct the covariance matrix.\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "5. Select k eigenvectors which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (k ≤ d).\n",
    "6. Construct a projection matrix W from the “top” k eigenvectors.\n",
    "7. Transform the d-dimensional input dataset X using the projection matrix W to obtain the new k-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d0d8b-4eb6-4f06-875e-13f38e8c8af9",
   "metadata": {},
   "source": [
    "## Q7.  formula is x(scaled) = (x- x(min))/(x(max) - x(min))\n",
    "1. here, x = [1,5,10,15,20] so x(min) = 1 and x(max) = 20\n",
    "2. for x = 1 , x(scaled) = (1-1)/(20-1) = 0\n",
    "3. for x = 5 , x(scaled) = (5-1)/(20-1) = 4/19 \n",
    "4. for x = 10 , x(scaled) = (10-1)/(20-1) = 9/19\n",
    "5. for x = 15 , x(scaled) = (15-1)/(20-1) = 14/19\n",
    "6. for x = 20 , x(scaled) = (20-1)/(20-1) = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92e4db-a23b-4ad3-a60a-d6de0e340ba3",
   "metadata": {},
   "source": [
    "## Q8. \n",
    "#### We know that more the features means more the dimensions,training with such features will result in high bias and will lead to overfitting.Therefore we say having high dimension is a curse. So we need to find a way to reduce the dimensions without eliminating each and every feature. Here comes the concept of Feature Selection Vs Feature Extraction! If we have high number of features the ‘Feature Selection’ technique takes in consideration subset of those features which are important or to eliminate those features which do not help classify our target. While in our case of ‘Feature Extraction’s’ our goal is of creating a new, smaller set of features that stills captures most of the useful information.We must know ‘Feature selection keeps a subset of the original features while feature extraction creates new ones’. “Feature extraction fills this requirement: it builds valuable information from raw data — the features — by reformatting, combining, transforming primary features into new ones… until it yields a new set of data that can be consumed by the Machine Learning models to achieve their goals.” In this way, we do completely eliminate those least important features like in feature selection & do not lose our original dataset completely. So Principal Component Analysis(PCA) is feature extraction technique meant to reduce the dimensions of our dataset.Note:We wont be going into detail of eigenvalues,eigenvectors involved in PCA, will just be showing how it works. The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other,while retaining the variation present in the dataset, up to the maximum extent. The same is done by transforming the variables to a new set of variables, which are known as the principal components (or simply, the PCs).\n",
    "### If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ed5cc-5b1b-4d0f-b33e-2c755556775c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
