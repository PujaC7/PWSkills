{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14622f4-7ed4-4e3b-9a98-679ec67349e9",
   "metadata": {},
   "source": [
    "##  Naive Approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbfcde-a53b-411c-a2cf-6d8a854feb42",
   "metadata": {},
   "source": [
    "## Q1.  A naive classifier model is one that does not use any sophistication in order to make a prediction, typically making a random or constant prediction. Such models are naive because they don't use any knowledge about the domain or any learning in order to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed4a15-db64-4b19-a598-08ad05e7efbd",
   "metadata": {},
   "source": [
    "## Q2.  The naive Bayes classifier assumes that all features in the input data are independent of each other, which is often not true in real-world scenarios. However, despite this simplifying assumption, the naive Bayes classifier is widely used because of its efficiency and good performance in many real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e25eb-09eb-40ae-8a5c-ac5619f8d029",
   "metadata": {},
   "source": [
    "## KNN:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602aa0c2-69b3-45b2-bf4d-85ed50f7c667",
   "metadata": {},
   "source": [
    "## Q10.  The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89074fd9-f1d5-4175-82d3-0a06bad69ee6",
   "metadata": {},
   "source": [
    "## Q11.  KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc38f1-b4f3-426f-8a5d-418eaa65357c",
   "metadata": {},
   "source": [
    "## Clustering:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38909424-4ff9-492b-ab4a-cde7081fda2c",
   "metadata": {},
   "source": [
    "## Q19.  In machine learning too, we often group examples as a first step to understand a subject (data set) in a machine learning system. Grouping unlabeled examples is called clustering. As the examples are unlabeled, clustering relies on unsupervised machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83e3ad-6fb2-4cd8-a172-111f58b60f3d",
   "metadata": {},
   "source": [
    "## Q20.  k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of 'K'. Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dae37a-6913-45bd-8d66-aca157733b02",
   "metadata": {},
   "source": [
    "## Anomaly Detection:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f9955-4b61-440b-89de-99d81cb9f954",
   "metadata": {},
   "source": [
    "## Q27.  Anomaly detection is a process of finding those rare items, data points, events, or observations that make suspicions by being different from the rest data points or observations. Anomaly detection is also known as outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc813e-90d1-49b9-a2dd-a27015b6a6f8",
   "metadata": {},
   "source": [
    "## Q28.  The main difference between supervised and unsupervised anomaly detection is the approach involved, where supervised approach makes use of predefined algorithms and AI training, while unsupervised approach uses a general outlier-detection mechanism based on pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e57486-9321-42aa-937b-49469c62ab54",
   "metadata": {},
   "source": [
    "## Dimension Reduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0dd907-b0fd-4fb7-bf82-1a65ac02237e",
   "metadata": {},
   "source": [
    "## Q34.  Dimensionality reduction is a machine learning (ML) or statistical technique of reducing the amount of random variables in a problem by obtaining a set of principal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1efe7-e57c-4e6f-a027-11e860c4252c",
   "metadata": {},
   "source": [
    "## Q35.  The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8246690c-017d-4d42-9b93-cb6e8cea9145",
   "metadata": {},
   "source": [
    "##  Feature Selection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f4dd2-c92a-457a-84e7-29f43ca6a00b",
   "metadata": {},
   "source": [
    "##  Q40.   Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a4219-78e5-4c1b-900d-e28cf5806d58",
   "metadata": {},
   "source": [
    "## Q41.  Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In embedded methods the feature selection is an integral part of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92aba6-7bd1-46eb-a705-7c896aa09d72",
   "metadata": {},
   "source": [
    "## Data Drift Detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8ff4c-787f-438b-82f3-931ec2a0c260",
   "metadata": {},
   "source": [
    "## Q46.  Data drift refers to the changing distribution of the data to which the model is applied. Concept drift refers to a changing underlying goal or objective for the model. Both data drift and concept drift can lead to a decline in the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5f372-2cd6-4913-8f62-60a02061088b",
   "metadata": {},
   "source": [
    "## Q47.  Data drift is one of the top reasons model accuracy degrades over time. For machine learning models, data drift is the change in model input data that leads to model performance degradation. Monitoring data drift helps detect these model performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740b40a-59d8-4e6b-8cda-e8a27a5ef840",
   "metadata": {},
   "source": [
    "## Data Leakage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac0225-82be-4b12-b36f-8cb2edf7ab14",
   "metadata": {},
   "source": [
    "##  Q51. Leakage occurs when information about the target label or number is introduced during learning that would not be lawfully accessible during actual use. The most fundamental example of data leakage would be if the true label of a dataset was included as a characteristic in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7281886-62c1-46b9-be5e-c66545e0cb06",
   "metadata": {},
   "source": [
    "## Q52.  A data leak is when information is exposed to unauthorized people due to internal errors. This is often caused by poor data security and sanitization, outdated systems, or a lack of employee training. Data leaks could lead to identity theft, data breaches, or ransomware installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba533f-9e0f-4bc4-9efa-585f0b5205fa",
   "metadata": {},
   "source": [
    "## Cross Validation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4775739-0b81-4760-9e8c-ab52e1dbe4ea",
   "metadata": {},
   "source": [
    "##  Q57.  Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97914fa0-f8d9-4e48-99e6-4ac530456456",
   "metadata": {},
   "source": [
    "## Q58.   The main advantage of cross-validation is that it provides an estimate of the performance of the model on new data, which is important for assessing the model's generalizability. It also helps to avoid overfitting, which is a common problem in machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
