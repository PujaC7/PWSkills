{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d6cfea-3306-49c6-b60c-a2eb400b4410",
   "metadata": {},
   "source": [
    "## Q1.  The coefficient of determination, or R2 , is a measure that provides information about the goodness of fit of a model. In the context of regression it is a statistical measure of how well the regression line approximates the actual data.\n",
    "### R 2 = 1 − sum squared regression (SSR) total sum of squares (SST) , = 1 − ∑ ( y i − y i ^ ) 2 ∑ ( y i − y ¯ ) 2 . The sum squared regression is the sum of the residuals squared, and the total sum of squares is the sum of the distance the data is away from the mean all squared.\n",
    "### R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd52af6-d808-41f9-b938-dc4a1a82ce08",
   "metadata": {},
   "source": [
    "## Q2.  Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected.\n",
    "### The most vital difference between adjusted R-squared and R-squared is simply that adjusted R-squared considers and tests different independent variables against the model and R-squared does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd460d-9ae1-47b4-b146-b4a4c55e6dcf",
   "metadata": {},
   "source": [
    "## Q3.  Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85cb20-e007-40a0-87b3-a28aea553d1e",
   "metadata": {},
   "source": [
    "## Q4.  RMSE is the Root of the Mean of the Square of Errors and MAE is the Mean of Absolute value of Errors. Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable\n",
    "### MAE is the average distance between the real data and the predicted data, but fails to punish large errors in prediction. MSE measures the average squared difference between the estimated values and the actual value. L1 and L2 Regularization is a technique used to reduce the complexity of the model. and RMSE is the square root of MSE\n",
    "### Usually the metrics used are the Mean Average Error (MAE), the Mean Squared Error (MSE) or the Root Mean Squared Error (RMSE). In short, MAE evaluates the absolute distance of the observations (the entries of the dataset) to the predictions on a regression, taking the average over all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf45f6-b892-484c-8e2d-ddf357e92ddf",
   "metadata": {},
   "source": [
    "## Q5.  advantages and disadvantages of MSE: Advantage: The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on theses errors due to the squaring part of the function. Disadvantage: If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\n",
    "### advantages of RMSE over MSE: MSE is highly biased for higher values. RMSE is better in terms of reflecting performance when dealing with large error values. RMSE is more useful when lower residual values are preferred.\n",
    "### disadvantages of RMSE: One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly. RMSE increases with an increase in the size of the test sample. This is an issue when we calculate the results on different test samples.\n",
    "### Disadvantages of MAE: i) Beacuse of its non-differentiable nature of graphs, will need various optimizers like gradient descent to make them differentiable. ii) Bigger error terms are failed to be punished. Mean of the squared distances between actual and predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641db00a-4dcb-477f-8bb3-36698bfd004e",
   "metadata": {},
   "source": [
    "## Q6.  The LASSO method regularizes model parameters by shrinking the regression coefficients, reducing some of them to zero. The feature selection phase occurs after the shrinkage, where every non-zero value is selected to be used in the model\n",
    "### Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization.\n",
    "### Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe530d2-2dc4-411f-9d64-277e5ac0d351",
   "metadata": {},
   "source": [
    "## Q7.  In short, Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "### Examples of regularization, included; K-means: Restricting the segments for avoiding redundant groups. Neural networks: Confining the complexity (weights) of a model. Random Forest: Reducing the depth of tree and branches (new features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6ecf5-9528-4e2b-81d2-6b77f75748a3",
   "metadata": {},
   "source": [
    "## Q8.  Disadvantages\n",
    "- They include all the predictors in the final model.\n",
    "- They are unable to perform feature selection.\n",
    "- They shrink the coefficients towards zero.\n",
    "- They trade the variance for bias.\n",
    "### Regularization does NOT improve the performance on the data set that the algorithm used to learn the model parameters (feature weights). However, it can improve the generalization performance, i.e., the performance on new, unseen data, which is exactly what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd89bd-2f8d-4b1a-878e-2a1fe5d2def6",
   "metadata": {},
   "source": [
    "## Q9.  MAE is a fundamental and most used evaluation metric for regression problems. Here we calculate the difference between the actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b081c-21cc-4daa-9570-7e090f1588e2",
   "metadata": {},
   "source": [
    "## Q10.  Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
