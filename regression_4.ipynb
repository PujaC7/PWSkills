{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432abb52-c125-4f0a-877c-ccb7f5b88580",
   "metadata": {},
   "source": [
    "## Q1.  Lasso regression algorithm is defined as a regularization algorithm that assists in the elimination of irrelevant parameters, thus helping in the concentration of selection and regularizes the models. Lasso models can be evaluated using various metrics such as RMSE and R-Square.\n",
    "### Ridge and Lasso Regression. Lasso Regression is different from ridge regression as it uses absolute coefficient values for normalization. As loss function only considers absolute coefficients (weights), the optimization algorithm will penalize high coefficients. This is known as the L1 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d938fb-5c22-45e1-8b11-427093a93c43",
   "metadata": {},
   "source": [
    "## Q2.  The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12585343-58fd-43b7-9258-d953a3f8295c",
   "metadata": {},
   "source": [
    "## Q3.  Non-Zero Coefficients: In Lasso regression, the regularization penalty can lead to some of the coefficients being shrunk to zero, resulting in a sparse model. The number of non-zero coefficients can be used to evaluate the effectiveness of the regularization and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7e4f2-9b32-4565-88d1-06a246906c7c",
   "metadata": {},
   "source": [
    "## Q4.  A tuning parameter (Î»), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean.\n",
    "### Hyperparameter tuning is an essential part of controlling the behavior of a machine learning model. If we don't correctly tune our hyperparameters, our estimated model parameters produce suboptimal results, as they don't minimize the loss function. This means our model makes more errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee787213-b514-48e5-a87e-50cea7f1ee00",
   "metadata": {},
   "source": [
    "## Q5.  If we have a specific problem in mind, just tell us what it is. But, in a general manner, the answer would be NO.\n",
    "### Regularization with a lasso penalty is an advantageous in that it reduces some unknown parameters in linear regression models toward exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby selecting the number of basis functions ef- fectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443379d-52f1-4601-8dec-c504e77fc08f",
   "metadata": {},
   "source": [
    "## Q6.  Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca7948-c523-4405-8bae-7e321fa9e61c",
   "metadata": {},
   "source": [
    "## Q7.  Lasso Regression\n",
    "Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38986bb4-290b-4f2e-9ea2-ed2da60baa6d",
   "metadata": {},
   "source": [
    "## Q8.  The best cross-validation score is obtained for the 0.4 value of lambda. This is your optimal value of lambda. This is how we choose the estimated best model with optimal hyper-parameter values. Use this same process with different types of algorithms like Ridge, LASSO, Elastic-Net, Random Forests, and Boosted trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
