{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cb99f5-f21f-415a-8603-c5e52c9a0cc4",
   "metadata": {},
   "source": [
    "## Q1.   The curse of dimensionality is a common problem in machine learning, where the performance of the model deteriorates as the number of features increases. This is because the complexity of the model increases with the number of features, and it becomes more difficult to find a good solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed6186-3d05-4228-8f63-8453843fb92e",
   "metadata": {},
   "source": [
    "## Q2.  As the dimensionality increases, the number of data points required for good performance of any machine learning algorithm increases exponentially. The reason is that, we would need more number of data points for any given combination of features, for any machine learning model to be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a67b2f-e95e-40b5-9362-54bd5fd18e04",
   "metadata": {},
   "source": [
    "## Q3.  The higher the dimensions, the more difficult it will be to sample from because the sampling loses its randomness. It becomes harder to collect observations if there are plenty of features. These dimensions make all observations in the dataset to be equidistant from all other observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e0ad3-a662-4a57-9e84-dbd5c16590de",
   "metadata": {},
   "source": [
    "## Q4.  Feature Selection vs Dimensionality Reduction\n",
    "While both methods are used for reducing the number of features in a dataset, there is an important difference. Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef28d3b1-1d38-4c79-ae71-df29859895bf",
   "metadata": {},
   "source": [
    "## Q5.  Disadvantages Of Dimensionality Reduction\n",
    "- We lost some data during the dimensionality reduction process, which can impact how well future training algorithms work.\n",
    "- It may need a lot of processing power.\n",
    "- Interpreting transformed characteristics might be challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002305bf-b22f-4383-a7ea-418c3db20ac4",
   "metadata": {},
   "source": [
    "## Q6.  Overfitting and Underfitting\n",
    "There is a relationship between 'd' and overfitting which is as follows: 'd' is directly proportional to overfitting i.e. as the dimensionality increases the chances of overfitting also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee98216-4903-47dc-8bc2-74c6f92649ae",
   "metadata": {},
   "source": [
    "## Q7.  Determining the optimal number of dimensions for data reduction depends on the specific dimensionality reduction technique being used and the goals of the analysis. However, here are a few general approaches:\n",
    "\n",
    "Variance Retained: Plotting the cumulative explained variance against the number of dimensions can help identify the point where adding more dimensions does not significantly increase the retained variance. Typically, a threshold (e.g., 90% variance retained) is set, and the corresponding number of dimensions is chosen.\n",
    "\n",
    "Scree Plot or Eigenvalue Analysis: For techniques like Principal Component Analysis (PCA), plotting the eigenvalues in descending order can reveal an \"elbow\" in the plot. The number of dimensions at the elbow point is often considered as a reasonable choice.\n",
    "\n",
    "Reconstruction Error: Techniques like Singular Value Decomposition (SVD) or Autoencoders can measure the reconstruction error when reducing dimensions. The number of dimensions that minimizes the reconstruction error can be chosen as the optimal number.\n",
    "\n",
    "Domain Expertise: Depending on the specific application and knowledge of the domain, one might have prior knowledge about the expected number of relevant dimensions. This expertise can guide the selection of the optimal number of dimensions.\n",
    "\n",
    "It's important to note that there is no universally \"correct\" or \"optimal\" number of dimensions for all scenarios. The choice depends on the context, data, and desired trade-offs between dimensionality reduction and information loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
