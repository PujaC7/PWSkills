{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbb0eaf-6b60-442f-b8e2-96459221bafe",
   "metadata": {},
   "source": [
    "## Q1.  Word Embeddings in NLP is a technique where individual words are represented as real-valued vectors in a lower-dimensional space and captures inter-word semantics. Each word is represented by a real-valued vector with tens or hundreds of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b2532-9c32-4d43-89df-f79028982077",
   "metadata": {},
   "source": [
    "## Q2.  A recurrent neural network is a type of artificial neural network commonly used in speech recognition and natural language processing. Recurrent neural networks recognize data's sequential characteristics and use patterns to predict the next likely scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef2faf-490d-45a7-a232-ffbff3b9c8cd",
   "metadata": {},
   "source": [
    "## Q3.  Encoder-decoder architecture is used in machine translation, text summarization, and image captioning tasks. Encoder compresses input to fixed-length vector; decoder generates output from it. It can be implemented using RNNs or transformer networks and trained using input-output pairs to learn to map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d40cb-c14d-4fa9-b625-bd5734da6a4e",
   "metadata": {},
   "source": [
    "## Q4.  Attention mechanisms can be useful for handling long input sequences, as they allow the model to selectively focus on the most relevant parts of the input. They can also be used to improve the interpretability of a model by providing a visual representation of which parts of the input the model is paying attention to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3ced5-7152-44c0-8b65-1dac45dfaf88",
   "metadata": {},
   "source": [
    "## Q5.  Self-Attention. The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ee2a6-6a59-407d-9fa0-8f018cc02c90",
   "metadata": {},
   "source": [
    "## Q6.  Like recurrent neural networks (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, Transformers do not require that the sequential data be processed in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9327369-8219-4a32-a2fc-eda98c701a4a",
   "metadata": {},
   "source": [
    "## Q7.  A text-to-text Generative AI is an AI that Generates text based on text input. An example of a text-to-text Generative AI is ChatGPT, developed by OpenAI. Text generation uses machine learning, existing data and previous user input in generating responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100958eb-cc2b-465d-b191-1c010fcf9f9e",
   "metadata": {},
   "source": [
    "## Q8.  Creating dialogues, headlines, or ads through generative AI is commonly used in marketing, gaming, and communication industries. These tools can be used in live chat boxes for real-time conversations with customers or to create product descriptions, articles, and social media content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72bf429-d92c-4577-aca3-ab0ed9d524c0",
   "metadata": {},
   "source": [
    "## Q9.  Other problems of conversational AI platforms:\n",
    "- Regional jargon and slang.\n",
    "- Dialects not conforming to standard language.\n",
    "- Background noise distorting the voice of the speaker.\n",
    "- Unscripted questions that the virtual assistant or chatbot does not know to answer.\n",
    "- Unplanned responses by customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af26fb-cc3a-4c6e-a948-0d3e2243ba6a",
   "metadata": {},
   "source": [
    "## Q10.  in order to maintain conversational context and manage dialog state, conversational history will have to be included in the ChatML document submitted, in order for the model to be able answer contextual questions. These contextual questions can be answered by the model by leveraging prior dialog turns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdef56-efcf-4e26-9e67-afc57fd0c515",
   "metadata": {},
   "source": [
    "## Q11.  Intent recognition is the process of identifying and understanding a user's intention or goal behind a given text or speech input in a conversational AI system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351c9a9-5eb1-4d2d-82b6-0d3ef00eebcb",
   "metadata": {},
   "source": [
    "## Q12.  This type of relationship between embeddings is very useful for finding relations between words. With vector operations, it is possible to discover words used with similar contexts (“Rome” and “Paris”), solve word analogies and create visualizations of similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8c1df-a74d-4e31-a69c-26abf0ca15e6",
   "metadata": {},
   "source": [
    "## Q13.  Because of their internal memory, RNNs can remember important things about the input they received, which allows them to be very precise in predicting what's coming next. This is why they're the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdbdd7-cb46-4fce-9041-f09165888bef",
   "metadata": {},
   "source": [
    "## Q14.  Encoder-decoder architectures can handle inputs and outputs that both consist of variable-length sequences and thus are suitable for seq2seq problems such as machine translation. The encoder takes a variable-length sequence as input and transforms it into a state with a fixed shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e33432f-360c-46de-afe4-9d486d77ec7b",
   "metadata": {},
   "source": [
    "## Q15.  In machine translation, attention mechanism is used to align and selectively focus on relevant parts of the source sentence during the translation process. It allows the model to assign weights to more important words or phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46859329-a509-47a3-90d3-b1d1a6e5687a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
